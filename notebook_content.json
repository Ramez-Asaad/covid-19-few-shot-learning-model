{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORD-19 Few-Shot Learning Analysis\n",
    "\n",
    "This notebook implements few-shot learning techniques to analyze the CORD-19 research papers dataset. We'll use a pre-trained BERT model to perform few-shot classification of research papers into different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and preprocess the CORD-19 dataset\n",
    "def load_data(file_path, nrows=None):\n",
    "    df = pd.read_csv(file_path, nrows=nrows)\n",
    "    # Keep only rows with non-null abstracts\n",
    "    df = df.dropna(subset=['abstract'])\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data('metadata.csv', nrows=5000)  # Start with a subset for development\n",
    "print(f\"Loaded {len(df)} papers with abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define categories for few-shot classification\n",
    "categories = {\n",
    "    'treatment': [\n",
    "        \"This paper discusses treatment options for COVID-19 patients.\",\n",
    "        \"The study evaluates the effectiveness of antiviral drugs.\",\n",
    "        \"Clinical trials of various therapeutic interventions are presented.\"\n",
    "    ],\n",
    "    'vaccine': [\n",
    "        \"Development and testing of COVID-19 vaccines are described.\",\n",
    "        \"Immune response to vaccination is analyzed.\",\n",
    "        \"Vaccine efficacy studies and results are presented.\"\n",
    "    ],\n",
    "    'epidemiology': [\n",
    "        \"The spread and transmission patterns of the virus are studied.\",\n",
    "        \"Statistical analysis of infection rates and patterns is performed.\",\n",
    "        \"Population-level impacts of the pandemic are evaluated.\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class FewShotClassifier:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.category_embeddings = {}\n",
    "    \n",
    "    def prepare_categories(self, categories):\n",
    "        \"\"\"Compute embeddings for each category's examples\"\"\"\n",
    "        for category, examples in categories.items():\n",
    "            embeddings = self.model.encode(examples)\n",
    "            self.category_embeddings[category] = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify a single text using few-shot learning\"\"\"\n",
    "        # Get embedding for the input text\n",
    "        text_embedding = self.model.encode(text)\n",
    "        \n",
    "        # Calculate similarity with each category\n",
    "        similarities = {}\n",
    "        for category, category_embedding in self.category_embeddings.items():\n",
    "            similarity = cosine_similarity(\n",
    "                text_embedding.reshape(1, -1),\n",
    "                category_embedding.reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities[category] = similarity\n",
    "        \n",
    "        # Return the category with highest similarity\n",
    "        return max(similarities.items(), key=lambda x: x[1])\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = FewShotClassifier()\n",
    "classifier.prepare_categories(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classify papers and analyze results\n",
    "def classify_papers(df, classifier, sample_size=1000):\n",
    "    # Take a sample if needed\n",
    "    if len(df) > sample_size:\n",
    "        df_sample = df.sample(sample_size, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "    \n",
    "    results = []\n",
    "    for _, row in tqdm(df_sample.iterrows(), total=len(df_sample)):\n",
    "        category, confidence = classifier.classify(row['abstract'])\n",
    "        results.append({\n",
    "            'title': row['title'],\n",
    "            'category': category,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run classification\n",
    "results_df = classify_papers(df, classifier)\n",
    "\n",
    "# Display distribution of categories\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=results_df, x='category')\n",
    "plt.title('Distribution of Paper Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display average confidence per category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=results_df, x='category', y='confidence')\n",
    "plt.title('Confidence Scores by Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze high-confidence papers for each category\n",
    "def analyze_top_papers(results_df, n=5):\n",
    "    for category in results_df['category'].unique():\n",
    "        print(f\"\\nTop {n} papers for category: {category}\")\n",
    "        top_papers = results_df[results_df['category'] == category].nlargest(n, 'confidence')\n",
    "        for _, paper in top_papers.iterrows():\n",
    "            print(f\"Title: {paper['title']}\")\n",
    "            print(f\"Confidence: {paper['confidence']:.3f}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "analyze_top_papers(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Learning Results Analysis\n",
    "\n",
    "Our few-shot learning approach has classified the papers into three main categories:\n",
    "1. Treatment studies\n",
    "2. Vaccine research\n",
    "3. Epidemiological studies\n",
    "\n",
    "Key findings:\n",
    "- The distribution of papers across categories shows...\n",
    "- Classification confidence is highest for...\n",
    "- We can identify clear patterns in how different research topics are discussed\n",
    "\n",
    "This approach demonstrates the power of few-shot learning in understanding research paper content without extensive labeled training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 